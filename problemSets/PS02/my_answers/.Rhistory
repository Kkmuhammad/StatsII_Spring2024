dist100_coef <- -2.93
welldist100_coef <- -0.02
se_intercept <- 0.49
se_well_depth <- 0.47
se_dist100 <- 0.31
se_welldist100 <- 0.30
df <- 1000
test_statistic <- (welldist100_coef / se_welldist100)^2
p_value <- 1 - pf(test_statistic, df - 1)
intercept <- 0.08
well_depth_coef <- 0.86
dist100_coef <- -2.93
welldist100_coef <- -0.02
se_intercept <- 0.49
se_well_depth <- 0.47
se_dist100 <- 0.31
se_welldist100 <- 0.30
df <- 1000
test_statistic <- (welldist100_coef / se_welldist100)^2
p_value <- 1 - pf(test_statistic, df - 1)
intercept <- 0.08
well_depth_coef <- 0.86
dist100_coef <- -2.93
welldist100_coef <- -0.02
se_intercept <- 0.49
se_well_depth <- 0.47
se_dist100 <- 0.31
se_welldist100 <- 0.30
df <- 1000
test_statistic <- (welldist100_coef / se_welldist100)^2
p_value <- 1 - pf(test_statistic, df - 1)
intercept <- 0.08
well_depth_coef <- 0.86
dist100_coef <- -2.93
welldist100_coef <- -0.02
se_intercept <- 0.49
se_well_depth <- 0.47
se_dist100 <- 0.31
se_welldist100 <- 0.30
df <- 1000
test_statistic <- (welldist100_coef / se_welldist100)^2
p_value <- 1 - pf(test_statistic, df - 1)
intercept <- 0.08
well_depth_coef <- 0.86
dist100_coef <- -2.93
welldist100_coef <- -0.02
se_intercept <- 0.49
se_well_depth <- 0.47
se_dist100 <- 0.31
se_welldist100 <- 0.30
df <- 1000
test_statistic <- (welldist100_coef / se_welldist100)^2
intpt <- -18.1368
wt_coef <- 2.2980
# Weight for ewe lamb
ewe_lamb_wt <- 6
# Calculate predicted Fatness for ewe lamb
pred_fat_ewe <- intpt + wt_coef * ewe_lamb_wt
pred_fat_ewe
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("us_tweets.csv",
stringsAsFactors=FALSE,
encoding = "utf-8")
View(data)
# Insert code here
num_tweets
# Insert code here
num_tweets
# Insert code here
num_tweets <- nrow(data)
num_tweets
cat("Number of tweets:", num_tweets, "\n")
cat("Number of tweets:", num_tweets)
cat("Number of tweets:", num_tweets)
cat(data)
cat(num_tweets)
print(num_tweets)
print(nrow(data))
# Insert code here
#num_tweets <- nrow(data)
print(nrow(data))
# Insert code here
#num_tweets <- nrow(data)
print(data)
print(num_tweets)
ndata <- data[which(data$is_retweet==FALSE),] # Fill in the gaps
trump <- ndata[which(data$screen_name==realDonaldTrump), ]# Fill in the gaps
trump <- ndata[which(data$screen_name=="realDonaldTrump"), ]# Fill in the gaps
View(trump)
sum(grepl("!", trump$text, ignore.case = TRUE)) # Adapt this code as needed
sum(grepl("winning", trump$text, ignore.case = TRUE))
#sum(grepl("!", trump$text, ignore.case = TRUE))
sum(grepl("winning", trump$text, ignore.case = TRUE))
sum(grepl("employment", trump$text, ignore.case = TRUE))
sum(grepl("hoax", trump$text, ignore.case = TRUE))
sum(grepl("immigration", trump$text, ignore.case = TRUE))
print(tweets_exclamation)
tweets_exclamation <- sum(grepl("!", trump$text, ignore.case = TRUE))
print(tweets_exclamation)
keywords <- c("winning", "employment", "immigration","hoax")
keywords <- "\\bwinning\\b|\\bemployment\\b|\\bimmigration\\b|\\bhoax\\b"
num_keywords <- sum(grepl(keywords, trump$text, ignore.case = TRUE))
print(num_keywords)
tweets_exclamation <- sum(grepl("!", trump$text, ignore.case = TRUE))
print(tweets_exclamation)
keywords <- "\\bwinning\\b|\\bemployment\\b|\\bimmigration\\b|\\bhoax\\b"
num_keywords <- sum(grepl(keywords, trump$text, ignore.case = TRUE))
print(num_keywords)
#keywords <- c("winning", "employment", "immigration","hoax")
#pattern <-
#sum(grepl("winning", trump$text, ignore.case = TRUE))
#sum(grepl("employment", trump$text, ignore.case = TRUE))
#sum(grepl("hoax", trump$text, ignore.case = TRUE))
#sum(grepl("immigration", trump$text, ignore.case = TRUE))
# Adapt this code as needed
library(quanteda)
# create corpus
corpus <- corpus(data)
corpus
corpus
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_????() %>% # which function transforms to lower case?
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>% # which function transforms to lower case?
tokens_remove(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks, # fill in the blanks here
method = "lambda", # which method?
size = 2, # which size?
min_count = 5, # how many?
smoothing = 0.5
)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks, # fill in the blanks here
method = "count", # which method?
size = 2, # which size?
min_count = 5, # how many?
smoothing = 0.5
)
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>% # which function transforms to lower case?
tokens_remove(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks, # fill in the blanks here
method = "count", # which method?
size = 2, # which size?
min_count = 5, # how many?
smoothing = 0.5
)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# create corpus
corpus <- corpus(data)
corpus
# create tokens object
toks <- tokens(corpus,
include_docvars = TRUE) %>%
tokens_tolower() %>% # which function transforms to lower case?
tokens_remove(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
tokens_remove('amp', valuetype = 'fixed', padding = TRUE)
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_collocations(toks, # fill in the blanks here
method = "lambda", # which method?
size = 2, # which size?
min_count = 5, # how many?
smoothing = 0.5
)
toks <- tokens_compound(toks, pattern = col[col$z > 2]) # choose a z score
toks <- tokens_compound(toks, pattern = col[col$z > 2]) # choose a z score
toks <- tokens_remove(tokens(toks), "") # this code removes whitespace
toks <- tokens_compound(toks, pattern = col[col$z > 2,]) # choose a z score
toks <- tokens_remove(tokens(toks), "") # this code removes whitespace
# create dfm from tokens object
docfm <- dfm(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
docfm <- dfm_select(docfm, pattern = stopwords("en"), selection = "remove")
library(ggplot2)
# Trump plot
dfm_trump <- dfm_subset(docfm, screen_name == "realDonaldTrump") # Fill in the blanks
dfm_freq <- textstat_frequency(dfm_trump, n = 20) # Fill in the blanks
View(dfm_freq)
dfm_freq$feature <- with(dfm_freq, reorder(feature, -frequency))
dfm_freq$feature
ggplot(dfm_freq, aes(x = dfm_freq$feature, y = dfm_freq$frequency)) +
ggtitle("Keywords in Trump's tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
dfm_comparison <- dfm_subset(docfm, data$screen_name == "SpeakerPelosi") # Use the correct argument for subsetting here
set.seed(1234)
# Pelosi plot
dfm_pelosi <- dfm_subset(docfm, screen_name == "SpeakerPelosi") # Fill in the blanks
dfm_freq <- textstat_frequency(dfm_pelosi, n = 20) # Fill in the blanks
dfm_freq$feature <- with(dfm_freq, reorder(feature, -frequency))
ggplot(dfm_freq, aes(x = dfm_freq$feature, y = dfm_freq$frequency)) +
ggtitle("Keywords in Pelosi's tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(dfm_freq, aes(x = dfm_freq$feature, y = dfm_freq$frequency)) +
ggtitle("Keywords in Trump's tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(dfm_freq, aes(x = dfm_freq$feature, y = dfm_freq$frequency)) +
ggtitle("Keywords in Pelosi's tweets") +
geom_point() +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
dfm_comparison <- dfm_subset(docfm, screen_name %>% c("realDonaldTrump","SpeakerPelosi")) # Use the correct argument for subsetting here
dfm_comparison <- dfm_subset(docfm, screen_name %in% c("realDonaldTrump","SpeakerPelosi")) # Use the correct argument for subsetting here
set.seed(1234)
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name) # select the correct variable to group on
dfm_keyness <- dfm_group(dfm_comparison, groups = data$screen_name) # select the correct variable to group on
dfm_keyness <- dfm_group(dfm_comparison, groups = screen_name) # select the correct variable to group on
keyness_stat <- textstat_keyness(dfm_keyness, target = "realDonaldTrump") # choose the correct function
textplot_keyness(keyness_stat, labelsize = 3) # choose the correct function
#Trump keyness
head(keyness_stat, 30)
# Pelosi keyness
# your code here
keyness_stat <- textstat_keyness(dfm_keyness, target = "SpeakerPelosi") # choose the correct function
textplot_keyness(keyness_stat, labelsize = 3) # choose the correct function
head(keyness_stat, 30)
# Trump
trump_corp <- corpus_subset(corpus, screen_name %in% "realDonaldTrump")
trump_kwic1 <- kwic(trump_corp, pattern = phrase("witch hunt"), window = 5, case_insensitive = TRUE)
trump_kwic2 <- kwic(trump_corp, pattern = phrase("great"), window = 5, case_insensitive = TRUE)
trump_kwic3 <- kwic(trump_corp, pattern = phrase("ukraine"), window = 5, case_insensitive = TRUE)
head(trump_kwic1)
# Pelosi
# Your code here
head(pelosi_kwic1)
pelosi_corp <- corpus_subset(corpus, screen_name %in% "SpeakerPelosi")
pelosi_kwic1 <- kwic(pelosi_corp, pattern = phrase("women"), window = 5, case_insensitive = TRUE)
pelosi_kwic2 <- kwic(pelosi_corp, pattern = phrase("families"), window = 5, case_insensitive = TRUE)
pelosi_kwic3 <- kwic(pelosi_corp, pattern = phrase("bipartisan"), window = 5, case_insensitive = TRUE)
head(pelosi_kwic1)
View(pelosi_kwic3)
View(pelosi_kwic1)
View(pelosi_kwic2)
View(pelosi_kwic3)
View(trump_kwic1)
View(trump_kwic2)
View(trump_kwic3)
sent_dfm <- dfm(dfm_trump, dictionary = data_dictionary_LSD2015[1:2]) # Fill in the blanks
docvars(dfm_trump, "prop_negative") <- as.numeric(sent_dfm[,1] / ntoken(dfm_trump)) # Fill in the blanks
docvars(dfm_trump, "prop_positive") <- as.numeric(sent_dfm[,1] / ntoken(dfm_trump)) # Fill in the blanks
docvars(dfm_trump, "net_sentiment") <- # complete the code
docvars(dfm_trump, "date2") <- # use the lubridate package to parse the date
sent_plot <- ggplot(???, aes(x = ???, y = ???)) + # Fill in the blanks
docvars(dfm_trump, "net_sentiment") <- # complete the code
docvars(dfm_trump, "date2") <- # use the lubridate package to parse the date
sent_plot <- ggplot(dfm_trump, aes(x = data, y = net_sentiment)) + # Fill in the blanks
geom_smooth() +
theme_minimal()
docvars(dfm_trump, "net_sentiment") <- # complete the code
docvars(dfm_trump, "date2") <- # use the lubridate package to parse the date
sent_plot <- ggplot(docvars(dfm_trump), aes(x = data, y = net_sentiment)) + # Fill in the blanks
geom_smooth() +
theme_minimal()
docvars(dfm_trump, "net_sentiment") <- # complete the code
docvars(dfm_trump, "date2") <- # use the lubridate package to parse the date
sent_plot <- ggplot(docvars(dfm_trump), aes(x = data2, y = net_sentiment)) + # Fill in the blanks
geom_smooth() +
theme_minimal()
sent_dfm <- dfm(dfm_trump, dictionary = data_dictionary_LSD2015[1:2]) # Fill in the blanks
docvars(dfm_trump, "prop_negative") <- as.numeric(sent_dfm[,1] / ntoken(dfm_trump)) # Fill in the blanks
docvars(dfm_trump, "prop_positive") <- as.numeric(sent_dfm[,1] / ntoken(dfm_trump)) # Fill in the blanks
docvars(dfm_trump, "net_sentiment") <- docvars(dfm_trump, "prop_positive") - docvars(dfm_trump, "prop_negative") # complete the code
docvars(dfm_trump, "date2") <- lubridate::ymd_hms(dfm_trump@docvars$created_at)# use the lubridate package to parse the date
sent_plot <- ggplot(docvars(dfm_trump), aes(x = date2, y = net_sentiment)) + # Fill in the blanks
geom_smooth() +
theme_minimal()
sent_plot
docvars(dfm_trump, "prop_positive") <- as.numeric(sent_dfm[,2] / ntoken(dfm_trump)) # Fill in the blanks
docvars(dfm_trump, "net_sentiment") <- docvars(dfm_trump, "prop_positive") - docvars(dfm_trump, "prop_negative") # complete the code
docvars(dfm_trump, "date2") <- lubridate::ymd_hms(dfm_trump@docvars$created_at)# use the lubridate package to parse the date
sent_plot <- ggplot(docvars(dfm_trump), aes(x = date2, y = net_sentiment)) + # Fill in the blanks
geom_smooth() +
theme_minimal()
sent_plot
library(rvest)
library(polite)
url <- "https://www.gov.ie/en/publication/904d6-budget-2023-ministers-speeches/"#https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0"
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
View(page)
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>% # the links are in the list node with the "li" class
html_node("a") %>% # the "a" child node
html_attr(name = "href")
# get titles of pages
titles <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node("a") %>%
html_text() # extract text from href; trim whitespace
# get dates of pages
dates <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node(".item_createdate") %>%
html_text()
# create dataframe
final_data <- data.frame("url" = hyperlinks, "title" = titles, "date" = dates, stringsAsFactors = FALSE)
View(final_data)
library(rvest)
library(polite)
url <- "https://www.finance.gov.pk/" #https://www.gov.ie/en/publication/904d6-budget-2023-ministers-speeches/"#https://www.mfa.gr/en/current-affairs/statements-speeches/?page=0"
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
View(page)
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>% # the links are in the list node with the "li" class
html_node("a") %>% # the "a" child node
html_attr(name = "href")
# get titles of pages
titles <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node("a") %>%
html_text() # extract text from href; trim whitespace
# get dates of pages
dates <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node(".item_createdate") %>%
html_text()
# create dataframe
final_data <- data.frame("url" = hyperlinks, "title" = titles, "date" = dates, stringsAsFactors = FALSE)
library(rvest)
library(polite)
library(rvest)
library(polite)
url <- "https://www.finance.gov.pk/fb_archieve.html"
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>% # the links are in the list node with the "li" class
html_node("a") %>% # the "a" child node
html_attr(name = "href") # extract "href" for hyperlink reference
# get titles of pages
titles <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node("a") %>%
html_text() # extract text from href; trim whitespace
# get dates of pages
dates <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node(".item_createdate") %>%
html_text()
# create dataframe
final_data <- data.frame("url" = hyperlinks, "title" = titles, "date" = dates, stringsAsFactors = FALSE)
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
library(rvest)
library(polite)
url <- "https://www.finance.gov.pk/fb_archieve.html"
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>% # the links are in the list node with the "li" class
html_node("a") %>% # the "a" child node
html_attr(name = "href") # extract "href" for hyperlink reference
# get titles of pages
titles <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node("a") %>%
html_text() # extract text from href; trim whitespace
# get dates of pages
dates <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node(".item_createdate") %>%
html_text()
library(rvest)
library(polite)
url <- "https://www.finance.gov.pk/imf.html"
# Acquire the html locally (we use the polite package, because the website does not allow anonymous scraping)
page <- url %>%
bow() %>%
scrape() # parse page into XML structure
# get hyperlinks
hyperlinks <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>% # the links are in the list node with the "li" class
html_node("a") %>% # the "a" child node
html_attr(name = "href") # extract "href" for hyperlink reference
# get titles of pages
titles <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node("a") %>%
html_text() # extract text from href; trim whitespace
# get dates of pages
dates <- page %>%
html_node(".tcs") %>%
html_nodes(css = "li") %>%
html_node(".item_createdate") %>%
html_text()
# create dataframe
final_data <- data.frame("url" = hyperlinks, "title" = titles, "date" = dates, stringsAsFactors = FALSE)
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
View(climateSupport)
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
# Change baseline category for 'countries' and 'sanctions'
countries <- relevel(countries, ref = "20 of 192")
# Change baseline category for 'countries' and 'sanctions'
countries <- relevel(climateSupport$countries, ref = "20 of 192")
# Change baseline category for 'countries' and 'sanctions'
climateSupport$countries <- relevel(climateSupport$countries, ref = "20 of 192")
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data
load(url("https://github.com/ASDS-TCD/StatsII_Spring2024/blob/main/datasets/climateSupport.RData?raw=true"))
# Change baseline category for 'countries' and 'sanctions'
climateSupport$countries <- relevel(climateSupport$countries, ref = "20 of 192")
# Convert 'countries' to a factor with levels in the desired order
climateSupport$countries <- factor(climateSupport$countries, levels = c("20 of 192", "80 of 192", "160 of 192"))
# Convert 'sanctions' to a factor with levels in the desired order
climateSupport$sanctions <- factor(climateSupport$sanctions, levels = c("None", "5%", "15%", "20% of the monthly household costs given 2% GDP growth"))
# Fit logistic regression model
model <- glm(choice ~ countries + sanctions, data = climateSupport, family = binomial(link = "logit"))
# Summary output
summary(model)
# Convert 'countries' to a factor with levels in the desired order
countries <- factor(countries, levels = c("20 of 192", "80 of 192", "160 of 192"))
# Convert 'countries' to a factor with levels in the desired order
countries <- factor(climateSupport$countries, levels = c("20 of 192", "80 of 192", "160 of 192"))
# Convert 'sanctions' to a factor with levels in the desired order
sanctions <- factor(climateSupport$sanctions, levels = c("None", "5%", "15%", "20% of the monthly household costs given 2% GDP growth"))
# Fit logistic regression model
model <- glm(choice ~ countries + sanctions, data = climateSupport, family = binomial(link = "logit"))
# Summary output
summary(model)
